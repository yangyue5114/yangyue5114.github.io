<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Yue Yang, CS, SJTU, Shanghai Jiao Tong University">
<meta name="description" content="Yue Yang&#39;s home page">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Yue Yang&#39;s Homepage</title>
<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159069803-1', 'auto');
ga('send', 'pageview');
</script>
<!-- End Google Analytics -->
<!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
-->
</head>
<body>

<div id="layout-content" style="margin-top:25px">
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">
					<h1>Yue Yang <font face="Arial">    杨悦 </font></h1></div>
				<img src="./pic/others/email.png" height="22px">  yang-yue@sjtu.edu.cn<br>
				<br>
				<img src="./pic/others/google_scholar_logo.png" height="22px">  <a href="https://scholar.google.com/citations?user=6xUd24cAAAAJ&hl=en&oi=sra">Google Sholar</a><br>
				<br>
				<img src="./pic/others/github_logo.png" height="22px">  <a href="https://github.com/yangyue5114">Github</a><br>
				<br>
				<img src="./pic/others/location.png" height="22px"> Shanghai, China<br>

			</td>
			<td>
    <img src="./pic/yangyue.jpg" border="0" width="180"><br>
   </td>
		</tr><tr>
	</tr></tbody>
</table>

<!--<h2>Biography [<a href="./CV-JinYueming.pdf">CV</a>]</h2>-->
<h2>Biography </h2>
<p>
	Hi! I am now a 3rd-year Ph.D. student at Shanghai Jiao Tong University, jointly cultivated by Shanghai AI Lab. 
	Fortunately supervised by <a href="http://luoping.me/">Prof. Ping Luo</a> and co-advised by <a href="https://yuwangsjtu.github.io/">Prof. Yu Wang</a>. 
	<br>
	I am deeply passionate about developing AI solutions that are beneficial to society. 
	My current research focus is on AI Safety, LLM Agent, and Evaluations. The best way to reach me is through my email yang-yue@sjtu.edu.cn.
	<br>
	Previously, I received my bachelor’s degree in in Computer Science and Technology at Central South University. 
	At CSU, I worked under the supervision of <a href="https://scholar.google.com/citations?user=3odvjZ0AAAAJ&hl=en&oi=ao">Prof. Xiyao Liu</a> 
	and <a href="https://scholar.google.com/citations?user=bdFQARIAAAAJ&hl=en&oi=ao">Prof. Wenyuan Yang</a>.

	<br>
</p>


<!-- <h2>News</h2>
<ul>
	<li>
		[9/2024] 2 papers accepted by NeurIPS 2024! 
			<a href = "https://arxiv.org/pdf/2403.20194" target="_blank">ConvBench</a> from Kaipeng, 
			<a href = "https://arxiv.org/pdf/2406.08845" target="_blank">Rethinking Human Evaluation Protocol</a> from Tianle. Congrats!
	</li>
	<li>
		[5/2024] 2 papers accepted by ICML 2024! 
			Our position work <a href = "https://openreview.net/pdf?id=H9fNj8ivTy" target="_blank">Position: Towards Implicit Prompt For Text-To-Image Models</a> is available here
				and code is also released.
			Another work is <a href = "https://arxiv.org/pdf/2404.16006" target="_blank">MMT-Bench</a> from Wenqi. Congrats!
	</li>
	<li>
		[2/2024] Our work <a href = "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_DiffAgent_Fast_and_Accurate_Text-to-Image_API_Selection_with_Large_Language_CVPR_2024_paper.pdf" target="_blank">DiffAgent</a> on accurate selection in seconds via API calls by LLM agent is accepted in CVPR 2024. Congrats on Lirui!
	</li>
	<li>
		[12/2024] We release <a href = "https://arxiv.org/abs/2412.04447" target="_blank">EgoPlan-Bench2</a>, evaluating planning capabilities of MLLMs across various real-world scenarios.
	</li>
	<li>
		[07/2024] We release <a href = "https://arxiv.org/abs/2407.08683" target="_blank">SEED-Story</a> for Multimodal Long Story Generation based on SEED-X.
	</li>
	<li>
		[04/2024] We release <a href = "https://arxiv.org/abs/2404.14396" target="_blank">SEED-X</a>, the latest in our SEED series, which unifies multi-granularity comprehension and generation.
	</li>
	<li>
		[02/2024] <a href = "https://github.com/AILab-CVC/SEED-Bench" target="_blank">SEED-Bench</a> is accepted by CVPR 2024.
	</li>
	<li>
		[01/2024] <a href = "https://arxiv.org/abs/2310.01218" target="_blank">SEED-LLaMA</a> is accepted by ICLR 2024.
	</li>
	<li>
	[12/2023] We release  <a href = "https://chenyi99.github.io/ego_plan" target="_blank">EgoPlan-Bench</a>, which evaluates egocentric embodied planning of MLLMs.
	</li>
		<li>
		[11/2023] We release  <a href = "https://github.com/AILab-CVC/SEED-Bench" target="_blank">SEED-Bench-2</a>, evaluating the hierarchical capabilities of MLLMs.
		</li>
		<li>
		[10/2023] We release an online gradio demo of <a href = "https://10a4e7976e6fc2032c.gradio.live/" target="_blank">SEED-LLaMA</a>.
		</li>
		<li>
		[10/2023] We release the technical report of <a href = "https://arxiv.org/abs/2310.01218" target="_blank">SEED-LLaMA</a>, which is empowered by the improved SEED-2 tokenizer.
	</li>
		<li>
		[08/2023] We release <a href = "https://github.com/AILab-CVC/SEED-Bench" target="_blank">SEED-Bench</a>, the most comprehensive MLLM benchmark to date.
	</li>
		<li>
		[07/2023] We release our <a href = "https://github.com/AILab-CVC/SEED" target="_blank">SEED</a>. Stay tuned for more updates.
	</li>
	<li>
		[02/2023] Three papers were accepted by CVPR 2023.
	</li>
	<li>
		[07/2022] One paper was accepted by ECCV 2022.
	</li>
	<li>
		[03/2022] One paper was accepted by CVPR 2022 as oral.
	</li>
	<li>
		[11/2021] One paper was accepted by IEEE TIP.
	</li>
	<li>
		[03/2021] Two papers were accepted by CVPR 2021.
	</li>
	<li>
		[03/2019] One paper was accepted by CVPR 2019.
	</li>
</ul> -->




<h2> Selected Publications</h2>
<!-- ( *equal contribution   <sup>#</sup>corresponding author / project lead ) -->
( *equal contribution)
<ul>
	<li>
		Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping <br />
		<b>Yue Yang</b>, Shuibai Zhang, Wenqi Shao, Kaipeng Zhang, Yi Bin, Yu Wang, Ping Luo
		<br /> arXiv preprint, 2024.
		[<a href="https://arxiv.org/pdf/2410.08695">Paper</a>]
<!-- 		
		[<a href="https://github.com/TencentARC/Divot">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/Divot?style=social"> -->
		<br />
	  </li>
	<li>
		Convbench: A multi-turn conversation evaluation benchmark with hierarchical capability for large vision-language models <br />
		Shuo Liu, Kaining Ying, Hao Zhang, <b>Yue Yang</b>, Yuqi Lin, Tianle Zhang, Chuanhao Li, Yu Qiao, Ping Luo, Wenqi Shao, Kaipeng Zhang
		<br /> <b>NeurIPS Spotlight 2024 </b>.
		[<a href="https://arxiv.org/pdf/2403.20194">Paper</a>]
		<br />
	  </li>
	<li>
		Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing Reliability, Reproducibility, and Practicality <br />
		Tianle Zhang, Langtian Ma, Yuchen Yan, Yuchen Zhang, Kai Wang, <b>Yue Yang</b>, Ziyao Guo, Wenqi Shao, Yang You, Yu Qiao, Ping Luo, Kaipeng Zhang
		<br /> <b>NeurIPS 2024</b>.
		[<a href="https://arxiv.org/pdf/2406.08845">Paper</a>]
<!-- 		[<a href="https://github.com/TencentARC/Moto/">Code</a>]
		[<a href="https://chenyi99.github.io/moto/">Project</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/Moto?style=social"> -->
		<br />
	  </li>
	<li>
		Position: Towards Implicit Prompt For Text-To-Image Models <br />
		<b>Yue Yang</b>, Yuqi Lin, Hong Liu, Wenqi Shao, Runjian Chen, Hailong Shang, Yu Wang, Yu Qiao, Kaipeng Zhang, Ping Luo
		<br /> <b>ICML 2024</b>.
		[<a href="https://openreview.net/pdf?id=H9fNj8ivTy">Paper</a>]
<!-- 		[<a href="https://github.com/qiulu66/EgoPlan-Bench2/">Code</a>]
		[<a href="https://qiulu66.github.io/egoplanbench2/">Project</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/qiulu66/EgoPlan-Bench2?style=social"> -->
		<br />
	  </li>
	<li>
		Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi <br />
		Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, <b>Yue Yang</b>, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, Jiayi Lei, Quanfeng Lu, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Peng Gao, Yali Wang, Yu Qiao, Ping Luo, Kaipeng Zhang, Wenqi Shao
		<br /> <b>ICML 2024</b>.
		[<a href="https://arxiv.org/pdf/2404.16006">Paper</a>]
<!-- 		[<a href="https://github.com/TencentARC/SEED-Story">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/SEED-Story?style=social"> -->
		<br />
	  </li>
	<li>
		DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model <br />
		Lirui Zhao*, <b>Yue Yang*</b>, Kaipeng Zhang, Wenqi Shao, Yuxin Zhang, Yu Qiao, Ping Luo, Rongrong Ji
		<br /> <b>CVPR 2024</b>.
		[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_DiffAgent_Fast_and_Accurate_Text-to-Image_API_Selection_with_Large_Language_CVPR_2024_paper.pdf">Paper</a>]
<!-- 		[<a href="https://github.com/ChenYi99/EgoPlan">Code</a>]
		[<a href="https://chenyi99.github.io/ego_plan/">Project</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/ChenYi99/EgoPlan?style=social"> -->
		<br />
	  </li>
	<li>
		Align, Adapt and Inject: Audio-Guided Image Generation, Editing and Stylization <br />
		<b>Yue Yang</b>, Kaipeng Zhang, Yuying Ge, Wenqi Shao, Zeyue Xue, Yu Qiao, Ping Luo
		<br /> <b>ICASSP 2024</b>.
		[<a href="https://ieeexplore.ieee.org/abstract/document/10446362">Paper</a>]
<!-- 		[<a href="https://github.com/AILab-CVC/SEED-X">Code</a>]
		[<a href="https://arc.tencent.com/en/ai-demos/multimodal">Demo</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/AILab-CVC/SEED-X?style=social"> -->
		<br />
	  </li>
	  <li>
		Data Adaptive Traceback for Vision-Language Foundation Models in Image Classification <br />
		Wenshuo Peng, Kaipeng Zhang, <b>Yue Yang</b>, Hao Zhang, Yu Qiao
		<br /> <b>AAAI 2024</b>.
		[<a href="https://ojs.aaai.org/index.php/AAAI/article/view/28249">Paper</a>]
<!-- 		[<a href="https://github.com/AILab-CVC/SEED-Bench">Code</a>]
		[<a href="https://huggingface.co/datasets/AILab-CVC/SEED-Bench-2">Data</a>]
		[<a href="https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard">Leaderboard</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/AILab-CVC/SEED-Bench?style=social"> -->
		<br />
	  </li>
	  <li>
		Name Your Colour For the Task: Artificially Discover Colour Naming via Colour Quantisation Transformer <br />
		Shenghan Su, Lin Gu, <b>Yue Yang</b>, Zenghui Zhang, Tatsuya Harada
		<br /> <b>ICCV Oral 2024</b>.
		[<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Su_Name_Your_Colour_For_the_Task_Artificially_Discover_Colour_Naming_ICCV_2023_paper.html">Paper</a>]
<!-- 		[<a href="https://github.com/AILab-CVC/SEED/tree/main">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/AILab-CVC/SEED?style=social"> -->
		<br />
	  </li>
<!-- 	  <li>
		Planting a SEED of Vision in Large Language Model <br />
		<b>Yuying Ge*</b>, Yixiao Ge*, Ziyun Zeng, Xintao Wang, Ying Shan
		<br /> Technical Report, 2023.
		[<a href="https://arxiv.org/abs/2307.08041">Paper</a>]
		[<a href="https://github.com/AILab-CVC/SEED/tree/v1">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/AILab-CVC/SEED?style=social">
		<br />
	  </li>
	  <li>
		Policy Adaptation from Foundation Model Feedback <br />
		<b>Yuying Ge</b>, Annabella Macaluso, Li Erran Li, Ping Luo, Xiaolong Wang
		<br /> CVPR, 2023.
		[<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ge_Policy_Adaptation_From_Foundation_Model_Feedback_CVPR_2023_paper.pdf">Paper</a>]
		[<a href="ttps://geyuying.github.io/PAFF/">Project</a>]
		[<a href="https://github.com/geyuying/PAFF_code">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/geyuying/PAFF_code?style=social">
		<br />
	  </li>
	  <li>
		Learning Transferable Spatiotemporal Representations from Natural Script Knowledge <br />
		Ziyun Zeng*, <b>Yuying Ge*</b>, Xihui Liu, Bin Chen, Ping Luo, Shu-Tao Xia, Yixiao Ge
		<br /> CVPR, 2023.
		[<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_Learning_Transferable_Spatiotemporal_Representations_From_Natural_Script_Knowledge_CVPR_2023_paper.pdf">Paper</a>]
		[<a href="https://github.com/TencentARC/TVTS">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/TVTS?style=social">
		<br />
	  </li>
	  <li>
		MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval <br />
		<b>Yuying Ge</b>, Yixiao Ge, Xihui Liu, Alex Jinpeng Wang, Jianping Wu, Ying Shan, Xiaohu Qie and Ping Luo
		<br /> ECCV, 2022.
		[<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136950685.pdf">Paper</a>]
		[<a href="https://github.com/TencentARC/MCQ/blob/main/MILES.md">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/MCQ?style=social">
		<br />
	  </li>
	  <li>
		Bridging Video-text Retrieval with Multiple Choice Questions <br />
		<b>Yuying Ge</b>, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie and Ping Luo</i>
		<br /> CVPR, 2022 (oral).
		[<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ge_Bridging_Video-Text_Retrieval_With_Multiple_Choice_Questions_CVPR_2022_paper.pdf">Paper</a>]
		[<a href="MCQ.html">Project</a>]
		[<a href="https://github.com/TencentARC/MCQ">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/MCQ?style=social">
		<br />
	  </li>
	  <li>
		Parser-Free Virtual Try-on via Distilling Appearance Flows <br />
		<b>Yuying Ge</b>, Yibing Song, Ruimao Zhang, Chongjian Ge, Wei Liu, and Ping Luo
		<br /> CVPR, 2021.
		[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_Parser-Free_Virtual_Try-On_via_Distilling_Appearance_Flows_CVPR_2021_paper.pdf">Paper</a>]
		[<a href="https://github.com/geyuying/PF-AFN">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/geyuying/PF-AFN?style=social">
		<br />
	  </li>
	  <li>
		DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images <br />
		<b>Yuying Ge</b>, Ruimao Zhang, Xiaogang Wang, Xiaoou Tang, and Ping Luo
		<br /> CVPR, 2019.
		[<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Ge_DeepFashion2_A_Versatile_Benchmark_for_Detection_Pose_Estimation_Segmentation_and_CVPR_2019_paper.pdf">Paper</a>]
		[<a href="https://github.com/switchablenorms/DeepFashion2">Data</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/switchablenorms/DeepFashion2?style=social">
		<br />
	  </li> -->
	</ul>





</tbody></table>

	<h2><font> Education </font></h2>
<ul style="list-style-type:none">
	  <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font size="3"><meta charset="utf-8">
		  Ph.D., Department of Information and Communication Engineering, Shanghai Jiao Tong University, 2022 - now<br>
		  Bachelor, Central South University (CSU) (ranking 1/525), 2018 - 2022<br>
	  </font> </p>
</ul>

	<h2><font> Experiences </font></h2>
<ul style="list-style-type:none">
	  <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font size="3"><meta charset="utf-8">
<!-- 		  Senior Researcher in Tencent ARC Lab, 2024 - Present<br>
		  Senior Researcher in Tencent AI Lab, 2023 - 2024<br> -->
		  Intern in Shanghai AI Lab, 2022 - now <br>
<!-- 		  Intern in Tencent AI Lab, 2020 - 2021 <br>
		  Research Assistant in Multimedia Lab (MMLab), The Chinese University of Hong Kong, 2018 - 2019 <br>
		  Intern in SenseTime Research, 2017 - 2018 <br> -->

	  </font> </p>
</ul>

<h2><font> Academic Activities </font></h2>
<ul style="list-style-type:none">
	  <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font size="3"><meta charset="utf-8">
		  Reviewer for ICML<br>
<!-- 		  Organizer of DeepFashion2 Challenge <a href='https://competitions.codalab.org/competitions/22966'>Clothes Landmark Detection</a>
			  and <a href='https://competitions.codalab.org/competitions/22967'>Clothes Retrieval</a> in 2019, 2020<br>
		  Organizer of <a href='https://sites.google.com/view/cvcreative2020'>Third Workshop on Computer Vision for Fashion, Art and Design</a> in CVPR, 2020 <br>
		  Organizer of <a href='https://sites.google.com/view/cvcreative/home?authuser=0'>Second Workshop on Computer Vision for Fashion, Art and Design</a> in ICCV, 2019 <br> -->
	  </font> </p>
</ul>




<p align=right>
	<a class="pull-right" href="#">
	<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=zPFGAwps3dHeHg_L_f7k-PGV37THUtOVbgXT6ZedWOs&cl=ffffff&w=a"></script></center>
	</a>
</p>

<p><center><font>
        <br>&copy; Yue Yang | Last updated: Dec. 2021</font></center>
</p>

</div>
</body></html>
